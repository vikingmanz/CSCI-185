{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed10855",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Caleb Sweatt\n",
    "#CSCI 185\n",
    "#W1475087"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "7b32dd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total tokens is 298480\n",
      "Token count for each doc is 30086 33357 28843 36442 37060 39649 25893 26603 17864 22683\n",
      "Unique tokens for each doc is 3535 3992 3302 4076 4281 4758 2951 3615 3572 3368\n",
      "Total unique tokens is 37450\n",
      "Unique token count across all documents is 14328\n"
     ]
    }
   ],
   "source": [
    "#Needs the documents folder unzipped and each text file uploaded to jupyter home\n",
    "#Task 1\n",
    "from nltk.tokenize import word_tokenize\n",
    "import os\n",
    "file = open(\"d1.txt\")\n",
    "one = file.read()\n",
    "list_read = []\n",
    "list_read.append(one)\n",
    "token1 = nltk.word_tokenize(one)\n",
    "file2 = open(\"d2.txt\")\n",
    "two = file2.read()\n",
    "list_read.append(two)\n",
    "token2 = nltk.word_tokenize(two)\n",
    "file3 = open(\"d3.txt\")\n",
    "three = file3.read()\n",
    "list_read.append(three)\n",
    "token3 = nltk.word_tokenize(three)\n",
    "file4 = open(\"d4.txt\")\n",
    "four = file4.read()\n",
    "list_read.append(four)\n",
    "token4 = nltk.word_tokenize(four)\n",
    "file5 = open(\"d5.txt\")\n",
    "five = file5.read()\n",
    "list_read.append(five)\n",
    "token5 = nltk.word_tokenize(five)\n",
    "file6 = open(\"d6.txt\")\n",
    "six = file6.read()\n",
    "list_read.append(six)\n",
    "token6 = nltk.word_tokenize(six)\n",
    "file7 = open(\"d7.txt\")\n",
    "seven = file7.read()\n",
    "list_read.append(seven)\n",
    "token7 = nltk.word_tokenize(seven)\n",
    "file8 = open(\"d8.txt\")\n",
    "eight = file8.read()\n",
    "list_read.append(eight)\n",
    "token8 = nltk.word_tokenize(eight)\n",
    "file9 = open(\"d9.txt\")\n",
    "nine = file9.read()\n",
    "list_read.append(nine)\n",
    "token9 = nltk.word_tokenize(nine)\n",
    "file10 = open(\"d10.txt\")\n",
    "ten = file10.read()\n",
    "list_read.append(ten)\n",
    "token10 = nltk.word_tokenize(ten)\n",
    "count = len(token1)\n",
    "count2 = len(token2)\n",
    "count3 = len(token3)\n",
    "count4 = len(token4)\n",
    "count5 = len(token5)\n",
    "count6 = len(token6)\n",
    "count7 = len(token7)\n",
    "count8 = len(token8)\n",
    "count9 = len(token9)\n",
    "count10 = len(token10)\n",
    "print(\"Total tokens is\", count + count2 + count3 + count4 + count5 + count6 + count7 + count8 + count9 + count10)\n",
    "print(\"Token count for each doc is\", count, count2, count3, count4, count5, count6, count7, count8, count9, count10)\n",
    "lowercase = [token.lower() for token in token1]\n",
    "unique = set(lowercase)\n",
    "lowercase2 = [token.lower() for token in token2]\n",
    "unique2 = set(lowercase2)\n",
    "lowercase3 = [token.lower() for token in token3]\n",
    "unique3 = set(lowercase3)\n",
    "lowercase4 = [token.lower() for token in token4]\n",
    "unique4 = set(lowercase4)\n",
    "lowercase5 = [token.lower() for token in token5]\n",
    "unique5 = set(lowercase5)\n",
    "lowercase6 = [token.lower() for token in token6]\n",
    "unique6 = set(lowercase6)\n",
    "lowercase7 = [token.lower() for token in token7]\n",
    "unique7 = set(lowercase7)\n",
    "lowercase8 = [token.lower() for token in token8]\n",
    "unique8 = set(lowercase8)\n",
    "lowercase9 = [token.lower() for token in token9]\n",
    "unique9 = set(lowercase9)\n",
    "lowercase10 = [token.lower() for token in token10]\n",
    "unique10 = set(lowercase10)\n",
    "unique_total = []\n",
    "unique_total.extend(unique)\n",
    "unique_total.extend(unique2)\n",
    "unique_total.extend(unique3)\n",
    "unique_total.extend(unique4)\n",
    "unique_total.extend(unique5)\n",
    "unique_total.extend(unique6)\n",
    "unique_total.extend(unique7)\n",
    "unique_total.extend(unique8)\n",
    "unique_total.extend(unique9)\n",
    "unique_total.extend(unique10)\n",
    "unique_uniques = set(unique_total)\n",
    "print(\"Unique tokens for each doc is\", len(unique), len(unique2), len(unique3), len(unique4), len(unique5), len(unique6), len(unique7), len(unique8), len(unique9), len(unique10))\n",
    "print(\"Total unique tokens is\", len(unique) + len(unique2) + len(unique3) + len(unique4) + len(unique5) + len(unique6) + len(unique7) + len(unique8) + len(unique9) + len(unique10))\n",
    "print(\"Unique token count across all documents is\", len(unique_uniques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "53903879",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1 Answers\n",
    "#1a. doc 1 has 30086, doc 2 has 33357, doc 3 has 28843, doc 4 has 36442, doc 5 has 37060, doc 6 has 39649, doc 7 has 25893, doc 8 has 26603, doc 9 has 17864, doc 10 has 22683\n",
    "#total tokens in entire collection is 298480\n",
    "#1b. unique tokens from part a is 14328"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f3f8dba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens after stop words removed in each doc is 17760 21166 17071 22045 22615 23879 15902 16416 11225 14317\n",
      "Number of total tokens after stop words removed is 182396\n",
      "Number of unique tokens after stop words removed is 16487\n"
     ]
    }
   ],
   "source": [
    "#Task 2\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered = [w for w in token1 if not w.lower() in stop_words]\n",
    "unique_filter = set(filtered)\n",
    "filtered2 = [w for w in token2 if not w.lower() in stop_words]\n",
    "unique_filter2 = set(filtered2)\n",
    "filtered3 = [w for w in token3 if not w.lower() in stop_words]\n",
    "unique_filter3 = set(filtered3)\n",
    "filtered4 = [w for w in token4 if not w.lower() in stop_words]\n",
    "unique_filter4 = set(filtered4)\n",
    "filtered5 = [w for w in token5 if not w.lower() in stop_words]\n",
    "unique_filter5 = set(filtered5)\n",
    "filtered6 = [w for w in token6 if not w.lower() in stop_words]\n",
    "unique_filter6 = set(filtered6)\n",
    "filtered7 = [w for w in token7 if not w.lower() in stop_words]\n",
    "unique_filter7 = set(filtered7)\n",
    "filtered8 = [w for w in token8 if not w.lower() in stop_words]\n",
    "unique_filter8 = set(filtered8)\n",
    "filtered9 = [w for w in token9 if not w.lower() in stop_words]\n",
    "unique_filter9 = set(filtered9)\n",
    "filtered10 = [w for w in token10 if not w.lower() in stop_words]\n",
    "unique_filter10 = set(filtered10)\n",
    "total_unique_filter = []\n",
    "total_unique_filter.extend(unique_filter)\n",
    "total_unique_filter.extend(unique_filter2)\n",
    "total_unique_filter.extend(unique_filter3)\n",
    "total_unique_filter.extend(unique_filter4)\n",
    "total_unique_filter.extend(unique_filter5)\n",
    "total_unique_filter.extend(unique_filter6)\n",
    "total_unique_filter.extend(unique_filter7)\n",
    "total_unique_filter.extend(unique_filter8)\n",
    "total_unique_filter.extend(unique_filter9)\n",
    "total_unique_filter.extend(unique_filter10)\n",
    "unique_filtered = set(total_unique_filter)\n",
    "print(\"Number of tokens after stop words removed in each doc is\", len(filtered), len(filtered2), len(filtered3), len(filtered4), len(filtered5), len(filtered6), len(filtered7), len(filtered8), len(filtered9), len(filtered10))\n",
    "print(\"Number of total tokens after stop words removed is\", len(filtered) + len(filtered2) + len(filtered3) + len(filtered4) + len(filtered5) + len(filtered6) + len(filtered7) + len(filtered8) + len(filtered9) + len(filtered10))\n",
    "print(\"Number of unique tokens after stop words removed is\", len(unique_filtered))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ced03948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Question 2 answers\n",
    "#2a. Number of tokens after stop words removed is: doc 1 17760, doc 2 21166, doc 3 17071, doc 4 22045, doc 5 22615, doc 6 23879, doc 7 15902, doc 8 16416, doc 9 11225, doc 10 14317\n",
    "#Total tokens after stop words removed is 182396\n",
    "#2b. Total unique tokens after stop words removed is 16487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0a89af23",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The terms in vocabulary are: \n",
      " ['10' '11' '12' ... 'zephyrs' 'zone' 'zounds'] \n",
      "\n",
      "TF_IDF matrix: \n",
      "   (0, 11156)\t0.0013418856192342924\n",
      "  (0, 3963)\t0.0011407253709904933\n",
      "  (0, 9562)\t0.0013418856192342924\n",
      "  (0, 8915)\t0.0008872934301620662\n",
      "  (0, 4604)\t0.0006541141877466201\n",
      "  (0, 12189)\t0.0013418856192342924\n",
      "  (0, 4829)\t0.0005956794070028584\n",
      "  (0, 5362)\t0.0009979999034904563\n",
      "  (0, 7891)\t0.0013418856192342924\n",
      "  (0, 11032)\t0.0006541141877466201\n",
      "  (0, 3452)\t0.0011407253709904933\n",
      "  (0, 12450)\t0.0009979999034904563\n",
      "  (0, 470)\t0.0005434077144182301\n",
      "  (0, 2029)\t0.0009979999034904563\n",
      "  (0, 3510)\t0.0009979999034904563\n",
      "  (0, 4073)\t0.0008872934301620662\n",
      "  (0, 10262)\t0.0005434077144182301\n",
      "  (0, 9292)\t0.0011407253709904933\n",
      "  (0, 12035)\t0.0009979999034904563\n",
      "  (0, 988)\t0.0013418856192342924\n",
      "  (0, 4170)\t0.0011407253709904933\n",
      "  (0, 6360)\t0.0011407253709904933\n",
      "  (0, 4416)\t0.0007968396552466573\n",
      "  (0, 3005)\t0.0011407253709904933\n",
      "  (0, 9164)\t0.0006541141877466201\n",
      "  :\t:\n",
      "  (9, 2689)\t0.0006087165738786101\n",
      "  (9, 13191)\t0.0006087165738786101\n",
      "  (9, 10274)\t0.0006087165738786101\n",
      "  (9, 3730)\t0.0006087165738786101\n",
      "  (9, 5764)\t0.0006087165738786101\n",
      "  (9, 6672)\t0.0006087165738786101\n",
      "  (9, 4634)\t0.0012174331477572203\n",
      "  (9, 7676)\t0.0006087165738786101\n",
      "  (9, 9307)\t0.0006087165738786101\n",
      "  (9, 8685)\t0.0006087165738786101\n",
      "  (9, 7223)\t0.0006087165738786101\n",
      "  (9, 13119)\t0.09617721867282039\n",
      "  (9, 12947)\t0.0006087165738786101\n",
      "  (9, 8233)\t0.0006087165738786101\n",
      "  (9, 396)\t0.34818588025856495\n",
      "  (9, 7470)\t0.0006087165738786101\n",
      "  (9, 819)\t0.0006087165738786101\n",
      "  (9, 3728)\t0.0006087165738786101\n",
      "  (9, 10273)\t0.0018261497216358303\n",
      "  (9, 13065)\t0.0006087165738786101\n",
      "  (9, 1577)\t0.03165326184168772\n",
      "  (9, 3849)\t0.001605129276047889\n",
      "  (9, 11628)\t0.1448745445831092\n",
      "  (9, 12939)\t0.021305080085751354\n",
      "  (9, 307)\t0.06756753970052572\n"
     ]
    }
   ],
   "source": [
    "#Task 3\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(list_read)\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "print('The terms in vocabulary are: \\n', feature_names, \"\\n\")\n",
    "print('TF_IDF matrix: \\n', tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "22bc3e6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine similarity between documents is \n",
      " [[1.         0.72946613 0.73583784 0.770259   0.79669928 0.75791424\n",
      "  0.72230862 0.79729685 0.78409789 0.70623022]\n",
      " [0.72946613 1.         0.66525956 0.7126399  0.73046881 0.68667594\n",
      "  0.75745155 0.7328169  0.73437245 0.6601013 ]\n",
      " [0.73583784 0.66525956 1.         0.70838306 0.73164738 0.68733364\n",
      "  0.67336767 0.72624597 0.71419834 0.64581577]\n",
      " [0.770259   0.7126399  0.70838306 1.         0.76921663 0.72303565\n",
      "  0.73589728 0.76299183 0.75453351 0.69559487]\n",
      " [0.79669928 0.73046881 0.73164738 0.76921663 1.         0.74813174\n",
      "  0.72130775 0.7958372  0.80150793 0.70901435]\n",
      " [0.75791424 0.68667594 0.68733364 0.72303565 0.74813174 1.\n",
      "  0.68543967 0.76985797 0.75193148 0.67804681]\n",
      " [0.72230862 0.75745155 0.67336767 0.73589728 0.72130775 0.68543967\n",
      "  1.         0.72918983 0.71243264 0.65401409]\n",
      " [0.79729685 0.7328169  0.72624597 0.76299183 0.7958372  0.76985797\n",
      "  0.72918983 1.         0.83281273 0.73218329]\n",
      " [0.78409789 0.73437245 0.71419834 0.75453351 0.80150793 0.75193148\n",
      "  0.71243264 0.83281273 1.         0.73161253]\n",
      " [0.70623022 0.6601013  0.64581577 0.69559487 0.70901435 0.67804681\n",
      "  0.65401409 0.73218329 0.73161253 1.        ]]\n",
      "\n",
      "\n",
      "Ranked order of documents: [7 8 4 0 3 5 1 6 2]\n"
     ]
    }
   ],
   "source": [
    "#Task 4 \n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix);\n",
    "print('Cosine similarity between documents is \\n', cosine_similarities)\n",
    "print('\\n')\n",
    "num_rows, num_columns = cosine_similarities.shape\n",
    "ranked_order = np.argsort(-cosine_similarities[:num_rows-1, num_columns-1], axis=0)\n",
    "print('Ranked order of documents:', ranked_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b384d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
